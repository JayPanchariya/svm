{"cells":[{"cell_type":"markdown","metadata":{"id":"qPmhOuVYcG3F"},"source":["# Stumps, trees and forests\n","\n","** Ecole Centrale Nantes **\n","\n","** Diana Mateus **"]},{"cell_type":"markdown","metadata":{"id":"nrw54JTccG3H"},"source":["PARTICIPANTS: **(Fill in your names)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdy4QirrcG3J"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import stats"]},{"cell_type":"markdown","metadata":{"id":"6Atthmp5cG3K"},"source":["## 1. Decision stumps\n","A decision stump is a machine learning model consisting of a one-level decision tree. That is, it is a decision tree with one internal node (the root) which is immediately connected to the terminal nodes (its leaves). A decision stump makes a prediction based on the value of just a single input feature. Sometimes they are also called 1-rules [Wikipedia]\n","\n","***a)*** Run the provided code to generate and plot a toy dataset consisting of 2D points and 4 classes\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hW4QwWcycG3L"},"outputs":[],"source":["#a) Load and plot dataset, split in train and test sets\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import train_test_split\n","\n","n_classes = 4\n","X, y = make_blobs(n_samples=300, centers=n_classes,\n","                  random_state=0, cluster_std=1.0)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.get_cmap('rainbow', 4));\n","plt.colorbar();\n","\n","Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LkOD_MbbcG3M"},"source":["\n","**b)** Observe the functions **stump** and **split**.\n","\n","**stump** is a function that generates the parameters of a random axis-aligned split  according to the number of features of a given dataset. The function receives the data matrix Xtrain and returns\n","    - the index of one randomly chosen feature (one dimension)\n","    - as well as a randomly chosen threshold within the min and max values of the chosen feature.\n","\n","**split**  receives as input:\n","    - A dataset of points\n","    - the parameters generated by the stump function above.\n","The function then partitions the *dataset* in two subsets according to the threshold of the chosen dimension.\n","The output are two arrays, each containing the _indices_ of the points belonging to one or the other subset.\n","\n","**c)** Run the split function **several times**, and display the resulting subsets as 2D scatter plots with circles of different colors for each class. Use the provided plotting functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q2UQmyrdcG3M"},"outputs":[],"source":["from random import randint, uniform\n","\n","# b) Stumps and Split\n","\n","def stump(X):\n","    f = randint(0, X.shape[1]-1) #randomly select a feature/dimension randint()\n","    t = uniform(np.min(X[:,f]),np.max(X[:,f])) #sample from an uniform() between the min and max values of the feature\n","    return f,t\n","\n","\n","def split(X, f, t):\n","    ind_l, = np.where(X[:,f]<=t)\n","    ind_r, = np.where(X[:,f]>t)\n","    return ind_l, ind_r\n","\n","# c) Plotting a stump\n","\n","def plot_stump (Xtrain,ytrain,feat,th,ind_l,ind_r):\n","    n_classes=np.max(ytrain)+1;\n","    #print(n_classes)\n","    for c in range(n_classes):\n","        X0_c=Xtrain[np.where(ytrain==c), 0] #first coordinate of points belonging to class c\n","        X1_c=Xtrain[np.where(ytrain==c), 1] #second coordinate of points belonging to class c\n","        plt.scatter(X0_c,X1_c, s=50,alpha=0.5,cmap=plt.cm.get_cmap('rainbow', 4),label=str(c))\n","\n","    #plt.axis('equal')\n","\n","    #draw the points on the left and right child as circles around the original training dataset\n","    plt.scatter(Xtrain[ind_l, 0], Xtrain[ind_l, 1], c='none', edgecolor='r')\n","    plt.scatter(Xtrain[ind_r, 0], Xtrain[ind_r, 1], c='none', edgecolor='b')\n","\n","    #draw threshold line\n","    if feat == 0:\n","        plt.plot([th,th],[np.min(Xtrain[:,1]),np.max(Xtrain[:,1])])\n","    elif feat == 1:\n","        plt.plot([np.min(Xtrain[:,0]),np.max(Xtrain[:,0])],[th,th])\n","    leg = plt.legend();\n","\n","\n","feat, th = stump(Xtrain)\n","print('Stump parameters', feat, th)\n","ind_l,ind_r = split(Xtrain, feat, th)\n","print('Sizes: original set:', len(ytrain),\n","      ' left subset:' ,  ind_l.shape,\n","      ' right subset:', ind_r.shape)\n","\n","\n","plot_stump(Xtrain,ytrain,feat,th,ind_l,ind_r)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"26Un2mCecG3N"},"source":["**d)** Implement a function ***class_distributions*** that given a set of points reaching a node returns the posterior class distribution of the node (approximated as the normalized histogram with #points per class).\n","\n","``` python\n","def class_distributions(ytrain,num_classes):\n","```\n","\n","Compute the class distributions of:\n","    - of the original training set (before the split),\n","    - of each of the 2 subsets resulting from after the split has been applied.\n","\n","Plot the histograms of the children nodes\n","\n","**e)** Create a function to compute the ***information_gain*** of a split. The function should receive the full training dataset (Xtrain and ytrain) and the indices of two subsets resulting from the current split.\n","\n","``` python\n","def information_gain(ytrain,ind_left, ind_right,num_classes):\n","```\n","```Hint: ```  when computing the Entropy, ignore the classes with zero probabilities, and carry on summation using the same equation.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83El46MxcG3O"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1Hh9GtccG3O"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Jni23uJ2cG3P"},"source":["**f)** Create a function ***train*** to perfom a *randomized node optimization*. The function receives as parameters the full training dataset (Xtrain and ytrain) as well as the number of random stumps to try. The function will\n","- generate the desired number of stumps,\n","- split the dataset according to each stump,\n","- evaluate the ***information gain*** for each split\n","- choose and then return the parameters of the best stump.\n","\n","``` python\n","def train_stumps(Xtrain, ytrain, trials):\n","```\n","\n","Print the progress of the information gain during the training process\n","\n","**g)** Make predictions with the trained model and display the restuls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZ4RNQgHcG3P"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"x2sn16_dcG3Q"},"source":["## 3. Comparing classifiers on the Caltech 101 dataset\n","\n","Compare the performances of an SVM, a Decision Tree and Random Forest classifiers on the 10 classes subset of the **Caltech 101** dataset.\n","Focus here on the **multi-class** classification task (e.g. 5 classes).\n","Reuse the `loadImagesAndLabels()` and `buildDataset()` functions from the SVM lab course.\n","\n","\n","**a)** Load and split the dataset\n","\n","**b)** Train three classification models on the train dataset\n","- a Logistic Regression\n","- an SVM\n","- a single decision tree ``sklearn.tree.DecisionTree``\n","- a Random Forest ``sklearn.ensemble.RandomForestClassifier``\n","\n","Question: Which among these are natively multi-class classifiers?\n","\n","Print the accuracy on the train and test sets for each model. ``from sklearn import metrics``\n","\n","**c)** Can we use ROC curves directly in this case? How ?\n","\n","**d)** Do a gridsearch with a 5-fold crossvalidation varying the hyperparameters of each model (e.g., for the SVM the ``kernel``, the ``gamma`` and ``c``, and for the tree-based models  ``max_depth``, ``max_features``, ``min_samples_leaf``, and ``n_estimators``).\n","\n","``from sklearn.model_selection import GridSearchCV``\n","\n","**Hint**\n","```\n","tuned_parameters = [{'max_depth': [1, 5, 10],\n","                     'max_features': [1, 15, 30, 45, 60],\n","                     'n_estimators': [1, 25, 50, 75, 100],\n","                     'min_samples_leaf': [3,5,10]}]\n","                     \n","```\n","- What are the best parameters found to maximize the area under the curve in each case?\n","\n","**e)** Discuss the results and curves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2SeKPtZucG3R"},"outputs":[],"source":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"tf2","language":"python","name":"tf2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}